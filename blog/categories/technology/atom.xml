<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: technology | Artsy Engineering]]></title>
  <link href="http://artsy.github.io/blog/categories/technology/atom.xml" rel="self"/>
  <link href="http://artsy.github.io/"/>
  <updated>2017-04-14T19:25:52+00:00</updated>
  <id>http://artsy.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Artsy's Technology Stack, 2017]]></title>
    <link href="http://artsy.github.io/blog/2017/04/14/artsy-technology-stack-2017/"/>
    <updated>2017-04-14T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2017/04/14/artsy-technology-stack-2017</id>
    <content type="html"><![CDATA[<a name="History"></a>
<h1>History</h1>

<p>Artsy was <a href="http://www.nytimes.com/2012/10/09/arts/design/artsy-is-mapping-the-world-of-art-on-the-web.html">launched in 2012 as the "Art Genome Project"</a> and grew exponentially ever since.</p>

<p>By 2014 we had 230,000 works of art from 600 museums and institutions and launched our first business, a subscription service for commercial galleries, bringing over 80,000 works for sale and partnerships with 37 art fairs and a handful of benefit auctions. That year collectors from 82 countries inquired on over $5.5B of art.</p>

<p>By 2015 we doubled our "for sale" inventory and aggregated 4,000 of the world's leading galleries and 60 art fairs. We also launched two new businesses: commercial auctions and online media.</p>

<p>Finally, in 2016 we, again, doubled our paid gallery network size to become the largest gallery network in the world and grew to become the most-read online art publication as our highly engaging editorial traffic ballooned 320%. We also launched a platform to bid in live auctions and a consignments service with all major auction houses.</p>

<a name="The.Artsy.Business.in.2017"></a>
<h1>The Artsy Business in 2017</h1>

<p>Artsy in 2017 is a very wide platform and it can be challenging to characterize simply. But when you boil it down to its essence, Artsy offers information and a marketplace. Our written content and fair coverage keep people informed about the art world, and the Art Genome powers our tools for exploration. Through our partnerships with the major player in the art market, galleries and auction houses, we offer our users a unified platform for buying and selling art.</p>

<p>Internally we consider Artsy to have three businesses: <em>Auctions</em>, <em>Content</em> and <em>Listings</em>.</p>

<ul>
<li><p><em>Auctions</em>: Auction houses and charities use Artsy as a sales channel for a commission because collectors want to discover and buy art in a single, central platform that excels at surfacing the art they want from a global market.</p></li>
<li><p><em>Content</em>: Brands pay Artsy to reach the first art audience at scale by enabling evergreen content online and for offline engagement during art world events.</p></li>
<li><p><em>Listings</em>: Galleries, Fairs and Institutions subscribe to Artsy for a fee because we bring a very large audience of art collectors and enthusiasts to their virtual doors.</p></li>
</ul>


<p>The Artsy team is now 166 employees across three offices in New York, Berlin and London. The Engineering organization is now 28 engineers, including 4 leads, 3 directors and a CTO. In this post, we'd like to comprehensively cover what, and how we make the technical and human sides of Artsy businesses work.</p>

<!-- more -->




<center>
 <img src="/images/tech-2017/businesses.svg" style="width:100%;">
</center>


<a name="Organizational.Structures"></a>
<h1>Organizational Structures</h1>

<p>In 2016, we <a href="/blog/2016/03/28/artsy-engineering-organization-stack">updated the Engineering organization</a> to be oriented around product verticals for businesses. We used to focus more on practices to group engineers working with the same technologies across product teams to facilitate knowledge sharing and avoid redundant efforts.</p>

<p>Since then, web and mobile "practices" have largely been subsumed into the separate product teams. Mobile's increasing reliance on React Native has aligned nicely with web tooling. It no longer made sense to keep the teams separate, so where product teams used to have 2 separate sub-teams of engineers, they've now merged into 1.</p>

<p>The Platform "practice" has remained as a way to coordinate and share work among product teams, as well as monitor and upgrade Artsy's platform over time. Most platform engineers operate from within product teams, while a few focusing on data and infrastructure form a core, dedicated Platform team.</p>

<center>
 <img src="/images/tech-2017/engineering-teams.svg" style="width:100%;">
</center>


<a name="Artsy.Technology.Infrastructure.2017.-.Splitting.the.Monolith"></a>
<h1>Artsy Technology Infrastructure 2017 - Splitting the Monolith</h1>

<p></div></div></div>
<a href='/images/tech-2017/artsy-stack.svg'>
  <img src="/images/tech-2017/artsy-stack.svg" alt="The Artsy Tech Stack 2017" style="width:100%;">
</a>
<div class="article-container-single"><div class='meta-container'><header>&nbsp;</header></div><div class='content-container'><div class='entry-content'>
</p>

<a name="User.Facing"></a>
<h2>User Facing</h2>

<p>A lot of the user-facing focus is on being able to present interfaces with a quality worthy of art.</p>

<p>What you see today when you go to <a href="https://artsy.net">www.artsy.net</a> is a website built with <a href="http://ezeljs.com">Ezel.js</a>, which is a boilerplate for <a href="http://backbonejs.org">Backbone</a> projects running on <a href="https://nodejs.org">Node</a> and using <a href="http://expressjs.com">Express</a> and <a href="http://browserify.org">Browserify</a>. We used to have separate projects for mobile and desktop web, but they <a href="https://github.com/artsy/force/pull/890">are now merged</a>. The combined app is hosted on <a href="http://heroku.com">Heroku</a> and uses <a href="http://redis.io">Redis</a> for caching. Assets, including artwork images, are served from <a href="http://aws.amazon.com/s3">Amazon S3</a> via the <a href="http://aws.amazon.com/cloudfront">CloudFront CDN</a>. This <a href="https://github.com/artsy/force">code is open-source</a>.</p>

<p>What you see today when you open the <a href="https://itunes.apple.com/us/app/artsy-collect-and-bid-on-fine-art-design/id703796080?mt=8">Artsy iOS app</a> is a mix of Objective-C, Swift and React Native. Objective-C and Swift continue to provide a lot of over-arching cross-View Controller code. While individual representations of Artsy resources tend to be built in React Native. All of our React Native code uses Relay to handle API integration. This <a href="https://github.com/artsy/eigen">code is open-source</a>.</p>

<p>You can also find Artsy on <a href="http://alexa.artsy.net">Alexa</a> and <a href="http://assistant.artsy.net">Google Home</a>, which are both open-source Node.js applications. There is also an open-source <a href="https://github.com/artsy/emergence/">Apple TV</a> app built in Swift.</p>

<p>Our core API serves the public facets of our product, many of our own internal applications, and even <a href="https://developers.artsy.net">some of your own projects</a>. It's built with <a href="https://www.ruby-lang.org/en">Ruby</a>, <a href="http://rack.github.io">Rack</a>, <a href="http://rubyonrails.org">Rails</a>, and <a href="https://github.com/intridea/grape">Grape</a> serving primarily JSON. The API is hosted on <a href="http://aws.amazon.com/opsworks">AWS OpsWorks</a> and retrieves data from several <a href="http://www.mongodb.com">MongoDB</a> databases hosted with <a href="https://www.compose.io">Compose</a>. It also uses <a href="http://memcached.org">Memcached</a> for caching and <a href="https://redis.io">Redis</a> for background queues with <a href="https://github.com/mperham/sidekiq/">Sidekiq</a>. It runs background jobs with <a href="https://github.com/collectiveidea/delayed_job">delayed_job</a>. We used to employ <a href="http://lucene.apache.org/solr">Apache Solr</a> and even <a href="https://www.google.com/cse">Google Custom Search</a> for the many search functions, but have since consolidated on <a href="https://www.elastic.co">Elasticsearch</a>.</p>

<p>Most modern code for both the website and the iOS app use an orchestration layer which is powered by <a href="http://graphql.org">GraphQL</a> to streamline their data fetching and reduce front-end complexity. Our GraphQL server is an <a href="http://expressjs.com">Express</a> app, using <a href="https://github.com/graphql/express-graphql">express-graphql</a> to provide a single API end-point. The API does not access our data directly, but forwards requests to the core API or other services. We have been migrating shared display logic into the GraphQL server, to make it easier to build consistent clients. This <a href="https://github.com/artsy/metaphysics">code is open-source</a>.</p>

<p>Consistently, our front-end code <a href="/blog/2017/02/05/Front-end-JavaScript-at-Artsy-2017/">has moved towards</a> using React across all platforms along with introducing stricter JavaScript languages like TypeScript over CoffeeScript in order to provide better tooling.</p>

<p>We continue to have a <a href="https://developers.artsy.net">public HAL+JSON API</a> for external developers. This API is in active use for contemporary production services inside Artsy and the <a href="https://github.com/artsy/doppler">website is open-source</a>, too.</p>

<center>
 <img src="/images/tech-2017/languages.svg" style="width:100%;">
</center>


<a name="Partner-Facing"></a>
<h2>Partner-Facing</h2>

<p>The vast customer-facing business is powered by a Content Management System (CMS) for gallery and institutional partners. This CMS lets them upload and manage gallery shows, fair booths, create artists, and edit artwork metadata. All CMS components talk to our core API. We also have a number of CMS-like internal applications to manage partners, auctions, art genomes, configuring fairs or performing recurrent billing (we use Stripe for storing and charging credit cards and ACH) with invoicing.</p>

<p>CMS applications are based on stable, mature technologies like <a href="http://rubyonrails.org">Rails</a>, <a href="http://getbootstrap.com">Bootstrap</a>, <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a> and <a href="http://coffeescript.org">CoffeeScript</a>, and gradually adopts modern client-side technologies like <a href="https://facebook.github.io/react">React</a> and <a href="http://browserify.org">Browserify</a>. They share a lot of common infrastructure.</p>

<p>We have a generic image-processing service in-house, which uses <a href="http://rubyonrails.org">Rails</a>, <a href="https://github.com/mperham/sidekiq/">Sidekiq</a>, <a href="https://redis.io">Redis</a>, and <a href="https://github.com/rmagick/rmagick">RMagick</a> with <a href="http://www.imagemagick.org/script/index.php">ImageMagick</a>. It receives image processing requests from many Artsy applications and generates thumbnails, tiles and watermarks images on S3.</p>

<a name="Collector-Facing"></a>
<h2>Collector-Facing</h2>

<p>Collectors inquire on artworks and engage in conversations with partners. For this purpose we have built a generic messaging system that manages communications between different parties. It receives messages via API or e-mail, finds or creates a conversation based on the recipients and forwards them to the proper addresses in that conversation. Its doesn't assume anything about the contents of the messages, which makes it a generic system for any type of conversation. The conversations surface to our partners via CMS.</p>

<a name="Running.Auctions"></a>
<h2>Running Auctions</h2>

<p>The Auctions business began with doing the occasional benefit auctions for charities. Most of these auctions are online-only, timed sales. The initial version of our auction systems came together before we began our move to microservices, and so it is baked into our core API. Last year, we launched a live auction integration product to allow users to bid on works at commercial sales at the actual auction house sale rooms. The real-time requirements of this system required a rethinking of how we process our bids.</p>

<p>The core API for a commercial auction is a Scala micro-service that uses <a href="http://akka.io">Akka</a> for distributed computing. It stores information in an append-only storage engine, based on Akka Persistence, with a small library we developed called <a href="https://github.com/artsy/atomic-store">atomic-store</a>. Communication with external clients can either be done via a REST API, or via WebSockets. People visiting a Live Auction on the web are interacting with a <a href="https://medium.com/@mjackson/universal-javascript-4761051b7ae9#.ev1yd3juy">universal</a> <a href="https://facebook.github.io/react">React</a>+<a href="http://redux.js.org">Redux</a> JavaScript app, served from an <a href="http://expressjs.com">Express</a> server. Bidders visiting a Live Auction on iOS are interacting with a Swift application built with <a href="https://github.com/JensRavens/Interstellar">Interstellar</a>, <a href="https://github.com/daltoniam/starscream">Starscream</a> and <a href="https://github.com/SwiftyJSON/SwiftyJSON">SwiftyJSON</a>.</p>

<p>A more detailed overview of the Auctions stack can be found in <a href="/blog/2016/08/09/the-tech-behind-live-auction-integration">The Tech Behind Live Auction Integration</a>.</p>

<a name="Publishing"></a>
<h2>Publishing</h2>

<p>Our in-house editorial team and partners use an <a href="https://github.com/artsy/positron">open-source</a> platform called "Writer" (which we've built) to publish rich content across the web. Writer is split in two parts: the editorial-focused CMS and a JSON API that stores and distributes content separately from the rest of Artsy's stack.</p>

<p>Writer's frontend is built with <a href="http://ezeljs.com">Ezel.js</a>, which is a boilerplate for <a href="http://backbonejs.org">Backbone</a> projects running on <a href="https://nodejs.org">Node</a> and using <a href="http://expressjs.com">Express</a> and <a href="http://browserify.org">Browserify</a>. We also heavily use <a href="https://facebook.github.io/react">React</a> and write in <a href="http://coffeescript.org">CoffeeScript</a>. Writer's backend exposes <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>-based and <a href="http://graphql.org">GraphQL</a> APIs that are consumed by our applications.</p>

<p>You can see Writer being put to work when you see articles on <a href="https://www.artsy.net">www.artsy.net</a>, Facebook Instant Articles, Google AMP, RSS, Apple News, and email. We handle the distribution and display in all of these channels. We also support brand sponsorship deals and produce front-end heavy projects such as <a href="https://www.artsy.net/2016-year-in-art">Year in Art 2016</a>, and <a href="https://www.artsy.net/article/artsy-editorial-2015-the-year-in-art">Year in Art 2015</a>.</p>

<a name="Data.Pipeline"></a>
<h2>Data Pipeline</h2>

<p>Data generally flows from consumer applications and services into <a href="https://aws.amazon.com/redshift">AWS RedShift</a>. We use a set of <a href="https://github.com/ruby/rake">rake</a> tasks run on <a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Flow+Plugin">Jenkins</a> to move data from our several MongoDB and PostgreSQL databases to Redshift via <a href="https://aws.amazon.com/s3">S3</a>. These rake tasks shell out to <a href="https://www.postgresql.org/docs/9.3/static/sql-copy.html">psql</a> or <a href="https://docs.mongodb.com/manual/reference/program/mongoexport">mongo-export</a> to generate CSV files for a list of services and upload them to an S3 bucket, then load those CSV files plus others found in that bucket (placed there by other services) into Redshift. If a <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">Redshift copy</a> fails due to data changes we sample the CSV and generate a working schema from its contents.</p>

<p>We also store application usage data provided by <a href="https://segment.com/warehouses">Segment Warehouses</a> as well as data from vendors such as <a href="https://www.salesforce.com">Salesforce</a> and <a href="http://www.sailthru.com">Sailthru</a>.</p>

<p>For production data processing (such as recommendations), large-scale machine learning or even simpler parallel processing such as generating website sitemaps, we have our own Hadoop cluster configured and managed by <a href="https://www.cloudera.com/products/product-components/cloudera-manager.html">Cloudera Manager</a> and running on EC2. We leverage <a href="http://spark.apache.org">Apache Spark</a> and <a href="https://www.cloudera.com/products/open-source/apache-hadoop.html">Hadoop</a> with some <a href="http://oozie.apache.org">Ooozie</a> workflow scheduling. The same data pipeline that writes data to S3 also pumps data to HDFS with either Ruby code or <a href="http://sqoop.apache.org">Sqoop</a> and is read by Spark jobs written in Scala using <a href="https://hive.apache.org">Hive</a>. Spark has improved performance and capacity tenfold over our older in-house systems and we will be moving all lengthy processing implemented in Ruby to this system gradually.</p>

<a name="Analytics"></a>
<h2>Analytics</h2>

<p>For general data access and dashboards we rely on <a href="https://looker.com">Looker</a>. This system empowers all non-engineers to access all of our data. At the time of writing, there are 50 users running 3,500 queries a day against Redshift via Looker. We've found it expedient to pre-compute common denormalized views, and to create our own session rollups from raw pageviews and events for the additional flexibility it gives us in understanding user behavior.</p>

<p>For more in-depth work, we use <a href="https://ipython.org/notebook.html">Jupyter Notebooks</a> to connect to our Redshift cluster and by default import <a href="http://pandas.pydata.org">pandas</a>, <a href="http://scikit-learn.org/stable">sci-kit learn</a>, and <a href="http://matplotlib.org/api/pyplot_api.html">pyplot</a> for data analysis.</p>

<a name="Search"></a>
<h2>Search</h2>

<p>We completed our full migration from <a href="http://lucene.apache.org/solr">Solr</a> to <a href="https://www.elastic.co">Elasticsearch</a> in the last 18 months, and now use Elasticsearch across all front-ends. This ranges from our artwork filter interfaces through to our real-time artwork similarity features. Elasticsearch gives us high availability clustering features out of the box and easy horizontal scaling on demand.</p>

<a name="Platform.Services"></a>
<h2>Platform Services</h2>

<p>As Artsy's business has grown more complex, so has the data and concepts handled by its core API. We've begun supporting certain product areas with separate, dedicated API services, and even extracting existing API domains into separate services when practical. These services tend to expose simple <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>-ful HTTP APIs, maintain separate data sources, and even do their own <a href="/blog/2016/10/26/jwt-artsy-journey">authentication</a>. This has certain advantages:</p>

<ul>
<li>Each system can be deployed and scaled independently.</li>
<li>Each chooses the best-suited languages and technologies for its purpose.</li>
<li>Code bases remain more focused and developers' cognitive overhead is minimized.</li>
</ul>


<p>Balancing these out are some very real disadvantages:</p>

<ul>
<li>Development must sometimes touch multiple systems.</li>
<li>Some data is copied between services. These can become out-of-sync, though we always try to have a single <em>authoritative</em> source in such cases.</li>
<li>Deploys must be coordinated.</li>
</ul>


<p>At our size and complexity, a single code base is simply impractical. So, we've tried to be consistent in the coding, deployment, monitoring, and logging practices of these services. The more repeatable and disciplined our process, the less overhead is introduced by additional systems.</p>

<p>We've also explored alternate communication patterns, so systems aren't as dependent on each other's APIs. Recently we've begun publishing a stream of data events from our core systems that other systems can consume. Other systems can simply subscribe to the notifications they care about, so the source system doesn't need to be concerned about integrating with one more destination. After experimenting with <a href="https://kafka.apache.org">Kafka</a> but finding it hard to manage, we switched to <a href="https://www.rabbitmq.com">RabbitMQ</a> for this purpose. To provide consistency when publishing events we have <a href="https://github.com/artsy/artsy-eventservice">our own gem</a>.</p>

<a name="Operations"></a>
<h2>Operations</h2>

<p>All our recent AWS infrastructure is configured in code using <a href="https://www.terraform.io">Terraform</a>. This approach has allowed us to quickly replicate entire deployments along with their dependencies and has increased visibility into the state of our infrastructure across our teams. We started developing an open source <a href="https://www.docker.com">Docker</a> workflow toolkit named <a href="https://github.com/artsy/hokusai">Hokusai</a> in order to manage a containerized workflow, CI and deployment to <a href="https://kubernetes.io">Kubernetes</a>. Our Kubernetes clusters are managed using <a href="https://github.com/kubernetes/kops">Kops</a> and similarly provisioned using Terraform. This new workflow is reducing our dependence on Heroku, giving us more flexibility in our deployments and a more efficient use of server resources.</p>

<a name="Closing.Remarks"></a>
<h2>Closing Remarks</h2>

<p>Like any attempts at mapping something as large as the daily work for a thirty-ish person engineering team, <a href="https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation">the map is not the territory</a>. However, the exploration is worth the time it takes to keep notes for reading again in the next two years.</p>

<p>If you're interested in helping us make this an even longer post in two more years, or <em>more interestingly</em> shorter - we nearly always have a <a href="https://www.artsy.net/jobs">position open for engineers</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Artsy's Technology Stack, 2015]]></title>
    <link href="http://artsy.github.io/blog/2015/03/23/artsy-technology-stack-2015/"/>
    <updated>2015-03-23T00:00:00+00:00</updated>
    <id>http://artsy.github.io/blog/2015/03/23/artsy-technology-stack-2015</id>
    <content type="html"><![CDATA[<p>Artsy has now grown past 100 team members and our Engineering organization is now 20 strong. For a brief overview of what the company has accomplished in the last two years, check out our <a href="http://2013.artsy.net">2013</a> and <a href="http://2014.artsy.net">2014</a> reviews.</p>

<p>This is a good opportunity to describe our updated technology stack. Last time <a href="/blog/2012/10/10/artsy-technology-stack">we did this</a> was when Artsy launched publicly in 2012.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/stats.png'></center>


<p>Three years ago Artsy was a classic <a href="http://rubyonrails.org">Ruby-on-Rails</a> monolith with a handful of adjacent processes and tools. We've since broken it up into many independent services, and continue to heavily be a Ruby and JavaScript shop, using Rails where appropriate, with native code on mobile devices and some JVM-based experiments in micro-services.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/languages.png'></center>




<!-- more -->


<p>What you see today when you go to <a href="https://www.artsy.net">www.artsy.net</a> is a website built with <a href="http://ezeljs.com">Ezel.js</a>, which is a boilerplate for <a href="http://backbonejs.org">Backbone</a> projects running on <a href="https://nodejs.org">Node</a> and using <a href="http://expressjs.com">Express</a> and <a href="http://browserify.org">Browserify</a>. The <a href="https://github.com/artsy/force">CoffeeScript code</a> is open-source. The mobile version of www.artsy.net is <a href="https://m.artsy.net">m.artsy.net</a> and is built on the same technology. Both run on <a href="http://heroku.com">Heroku</a> and use <a href="http://redis.io">Redis</a> for caching. Assets, including artwork images, are served from <a href="http://aws.amazon.com/s3/">Amazon S3</a> via the <a href="http://aws.amazon.com/cloudfront">CloudFront CDN</a>.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/artsy.png'></center>


<p>These web applications talk to a private Ruby API built with <a href="https://github.com/intridea/grape">Grape</a>, that serves JSON. We also have a more modern and better designed <a href="https://developers.artsy.net">public HAL+JSON API</a>. For historical reasons, both are hosted side-by-side on top of a big Rails app that used to be our original monolith. The API service runs on <a href="http://aws.amazon.com/opsworks">AWS OpsWorks</a> and retrieves data from several <a href="http://www.mongodb.com">MongoDB</a> databases hosted with <a href="https://www.compose.io">Compose</a>. It also uses <a href="http://lucene.apache.org/solr">Apache Solr</a>, <a href="https://www.elastic.co">Elastic Search</a> and <a href="https://www.google.com/cse">Google Custom Search</a>. The API service also heavily relies on <a href="http://memcached.org">Memcached</a>.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/developers.png'></center>


<p>Our partners upload artworks, artists and other metadata via a home-grown content-management system built entirely from scratch in 2014 on Ruby-on-Rails. This was a deliberate <a href="http://chris.eidhof.nl/posts/boring-choice.html">"boring"</a> choice that continues to serve us very well. We have adopted a common model for admin-type apps with a shared component library and a reusable UI, all implemented as Rails engines. Using these components we are able to quickly and easily compose beautiful and useful applications - we have built dedicated systems to manage fairs and auctions. We standardized on related services as well - for example, our customers interact with us via <a href="https://www.intercom.io">Intercom</a>. We're also experimenting with some new technologies in our internal apps, notably <a href="http://facebook.github.io/react">React</a>.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/cms.png'></center>


<p>Our family of mobile applications includes <a href="http://iphone.artsy.net">Artsy for iOS</a>, which is a hybrid app written in Objective-C, and a <a href="https://github.com/artsy/eidolon">bidding kiosk</a>, written in Swift. Both are open-source <a href="https://github.com/artsy/eigen">here</a> and <a href="https://github.com/artsy/eidolon">here</a>.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/folio.jpg'></center>


<p>A lot of data, including the artwork similarity graph that powers The Art Genome Project, is processed offline by a generic job engine, written in Ruby or by <a href="http://aws.amazon.com/elasticmapreduce">Amazon Elastic MapReduce</a>. We take data snapshots from MongoDB, run jobs on the data and export data back to the database. Other recently rewritten services include image processing, which creates thumbnails, image tiles for deep zoom and watermarks high quality JPEGs. Several new applications use <a href="http://www.postgresql.org">PostgreSQL</a>.</p>

<p>Various front-ends pipe data to <a href="https://github.com/snowplow/snowplow">Snowplow</a> and <a href="https://segment.com">Segment</a>, which forwards events to <a href="https://keen.io">Keen</a>, <a href="http://www.google.com/analytics">Google Analytics</a>, <a href="https://mixpanel.com">MixPanel</a> and <a href="https://chartbeat.com">ChartBeat</a>. Some data is warehoused in <a href="http://aws.amazon.com/redshift">AWS Redshift</a> and <a href="http://www.postgresql.org">PostgreSQL</a> and may be analyzed offline using <a href="http://www.r-project.org">R</a> or <a href="http://ipython.org/notebook.html">iPython Notebooks</a>. We also have a <a href="https://github.com/etsy/statsd">Statsd</a> and <a href="http://graphite.wikidot.com">Graphite</a> system for tracking high volume, low-level counters. Finally, it's also fairly common to find a non-Engineer at Artsy in a read-only Rails console or in Redshift directly querying data.</p>

<p>We send millions of e-mails via <a href="http://sendgrid.com/">SendGrid</a> and <a href="https://mandrill.com">Mandrill</a> and use <a href="http://mailchimp.com">MailChimp</a> for manual campaigns.</p>

<p>Smaller systems usually start on <a href="https://dashboard.heroku.com">Heroku</a> and larger processes that perform heavier workloads usually end up on <a href="http://aws.amazon.com/opsworks">AWS OpsWorks</a>. Our systems are monitored by a combination of <a href="http://newrelic.com/">New Relic</a> and <a href="https://www.pingdom.com">Pingdom</a>. All of this is built, tested and continuously deployed with <a href="http://jenkins-ci.org">Jenkins</a>, <a href="https://semaphoreci.com">Semaphore</a>, and <a href="https://travis-ci.org">Travis-CI</a>.</p>

<center><img src='/images/2015-03-23-artsy-technology-stack-2015/gravity.png'></center>


<p>In terms of Engineering workflow we live in <a href="https://github.com">Github</a> and <a href="https://trello.com">Trello</a>. We tend to have a workflow similar to open-source projects with individuals owning components and services and the entire team contributing to them.</p>

<p>In 2015 we intend to complete our transformation into small independent services built with 10x growth in mind. We can then focus on maturing the Artsy platform both vertically and horizontally and enabling many new directions for our thriving businesses.</p>

<p>We hope you find this useful and will be happy to describe any detailed aspect of our system on this blog. We're always hiring, please e-mail <a href='mailto:jobs@artsy.net'>jobs@artsy.net</a> if you want to work with us. Finally, we welcome any questions here and look forward to answering them below!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Artsy's Technology Stack]]></title>
    <link href="http://artsy.github.io/blog/2012/10/10/artsy-technology-stack/"/>
    <updated>2012-10-10T21:21:00+00:00</updated>
    <id>http://artsy.github.io/blog/2012/10/10/artsy-technology-stack</id>
    <content type="html"><![CDATA[<p>The public launch of Artsy via the <a href="http://www.nytimes.com/2012/10/09/arts/design/artsy-is-mapping-the-world-of-art-on-the-web.html">New York Times</a> is a good opportunity to describe our current technology stack.</p>

<p>What you see when you go to <a href="http://artsy.net">Artsy</a> is a website built with <a href="http://backbonejs.org/">Backbone.js</a> and written in <a href="http://coffeescript.org/">CoffeeScript</a>. It renders JSON data from <a href="http://rubyonrails.org/">Ruby on Rails</a>, <a href="https://github.com/intridea/grape">Ruby Grape</a> and <a href="http://nodejs.org/">Node.js</a> services. Text search is powered by <a href="http://lucene.apache.org/solr/">Apache Solr</a>. We also have an <a href="https://developer.apple.com/devcenter/ios/index.action">iOS</a> application that talks to the same back-end Ruby API. We run all our web processes on <a href="http://www.heroku.com/">Heroku</a> and all job queues on <a href="http://aws.amazon.com/">Amazon EC2</a>. Our data store is <a href="http://www.mongodb.org/">MongoDB</a>, operated by <a href="https://mongohq.com/">MongoHQ</a> and we have some <a href="http://redis.io/">Redis</a> instances. Our assets, including images, are served from <a href="http://aws.amazon.com/s3/">Amazon S3</a> via the <a href="http://aws.amazon.com/cloudfront/">CloudFront CDN</a>. We heavily rely on <a href="http://memcached.org/">Memcached</a> Heroku addon and we use <a href="http://sendgrid.com/">SendGrid</a> and <a href="http://mailchimp.com/">MailChimp</a> to send e-mail. Systems are monitored by a combination of <a href="http://newrelic.com/">New Relic</a> and <a href="https://www.pingdom.com/">Pingdom</a>. All of this is built, tested and deployed with <a href="http://jenkins-ci.org/">Jenkins</a>.</p>

<p><img src="/images/2012-10-10-artsy-technology-stack/artsy-infrastructure.png"></p>

<p>In this post I'll go in depth in our current system architecture and tell you the story about how these parts all came together.</p>

<!-- more -->


<a name="Early.Prototypes"></a>
<h2>Early Prototypes</h2>

<p>Artsy early prototypes in 2010 consisted of a combination of PHP and Java web services running on JBoss and backed by a MySQL database. The system had more similarities with a large transactional banking application than a consumer website.</p>

<p>In early 2011 we rebooted the project on Ruby on Rails. RDBMS storage was replaced with NoSQL MongoDB. A <a href="http://www.10gen.com/presentations/MongoNYC-2012/Using-MongoDB-to-Build-Artsy">video</a> was recorded at MongoNYC 2012 that goes in depth into this specific choice.</p>

<a name="Artsy.Architecture.Today"></a>
<h2>Artsy Architecture Today</h2>

<p>Having only a handful of engineers, our goal has always been to keep the number of moving parts to an absolute minimum. With a few new engineers we were able to expand things a bit.</p>

<a name="Artsy.Website.Front-End"></a>
<h2>Artsy Website Front-End</h2>

<p>The Artsy website is a responsive <a href="http://backbonejs.org/">Backbone.js</a> application written in <a href="http://coffeescript.org/">CoffeeScript</a> and <a href="http://sass-lang.com/">SASS</a> and served from a Rails back-end. The generated JavaScript and CSS files are packaged and compressed with <a href="http://documentcloud.github.com/jammit/">Jammit</a> and deployed to Amazon S3. The Rails app itself is a traditional MVC system that bootstraps application data and mostly serves SEO needs, such as meta tags, escaped fragments and page titles. Once the basic data has been rendered though, Backbone routing takes over and you're now navigating a client-side browser app with pushState support as available, swapping frames and rendering views using JST templates and JSON data returned from the API.</p>

<a name="Core.API"></a>
<h2>Core API</h2>

<p>The website talks to the nervous system of Artsy, a RESTful API built in Ruby and <a href="https://github.com/intridea/grape">Grape</a>.</p>

<p>In the early days we did a ton of domain-driven design and spent a lot of time modeling concepts such as <em>artist</em> or <em>artwork</em>. The API has read and write behavior for all our domain concepts. Probably 70% of it is pure CRUD doing <a href="http://mongoid.org/">Mongoid</a> queries with a layer of access control in <a href="https://github.com/ryanb/cancan">CanCan</a> and cache partitioning and binding using <a href="http://confreaks.com/videos/986-goruco2012-from-zero-to-api-cache-w-grape-mongodb-in-10-minutes">Garner</a>.</p>

<a name="Search.Autocomplete"></a>
<h2>Search Autocomplete</h2>

<p>The first iteration of the website's text search was powered by <a href="https://github.com/artsy/mongoid_fulltext">mongoid_fulltext</a>. Today we run an <a href="http://lucene.apache.org/solr/">Apache Solr</a> master-slave environment hosted on EC2.</p>

<a name="Offline.Indexes"></a>
<h2>Offline Indexes</h2>

<p>The indexes that serve complex queries like related artists/artworks and filtered searches of artworks are all built offline. Our index-building system runs continuously, repeatedly pulling data from our production system to build the most out-of-date index. All of the most current indexes are imported back into production by a daily batch process and we swap the old indexes out atomically using <a href="https://github.com/aaw/mongoid_collection_snapshot">mongoid_collection_snapshot</a>.</p>

<p>One of such indexes a <em>similarity graph</em> that we query to produce most similar results on the website, other indexes serve filtering needs, etc. We run these processes nightly.</p>

<a name="Admin.Back-End.and.Partner.CMS"></a>
<h2>Admin Back-End and Partner CMS</h2>

<p>The Artsy CMS and the Admin system are two newer projects and serve the needs of our partners and our internal back-end needs, respectively. These are built on a thin <a href="http://nodejs.org">Node.js</a> server that proxies requests to our API using <a href="https://github.com/nodejitsu/node-http-proxy">node-http-proxy</a>. They consist of a client-side Backbone.js application with assets packaged with <a href="https://github.com/craigspaeth/nap">nap</a>. This is a lot like our website, but completely decoupled from the main Rails application and sharing the same technology for both client and server with CoffeeScript and <a href="http://jade-lang.com/">Jade</a>.</p>

<a name="Folio.Partner.App"></a>
<h2>Folio Partner App</h2>

<p>Artsy makes a free iOS application, called <a href="http://artsy.github.com/blog/categories/ios/">Folio</a>, which lets our partners display their inventory at art fairs.</p>

<p>Folio is a native iOS implementation. The interface is heavily skinned UIKit with CoreData for storage. Our network code was originally a thin layer on top of NSURLConnection, but for our forthcoming update, we’ve rewritten it to use <a href="https://github.com/AFNetworking/AFNetworking/">AFNetworking</a>. We manage external dependencies with <a href="https://github.com/CocoaPods/CocoaPods">CocoaPods</a>.</p>

<a name="Want.More.Specifics..Have.Questions."></a>
<h2>Want More Specifics? Have Questions?</h2>

<p>We hope you find this useful and are happy to describe any aspect of our system on this blog. Please ask questions below, we’ll be happy to answer them.</p>
]]></content>
  </entry>
  
</feed>
